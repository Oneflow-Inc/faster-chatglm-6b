# faster-chatglm-6b

`faster-chatglm-6b` æ˜¯ä¸€ä¸ªä½¿ç”¨ [OneFlow](https://github.com/Oneflow-Inc/oneflow) æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸ºåç«¯åŠ é€Ÿ[THUDM/chatglm-6b](https://huggingface.co/THUDM/chatglm-6b)è¿è¡Œçš„é¡¹ç›®ã€‚

![demo](images/demo.gif)

æ³¨ï¼šä¸Šå›¾ä¸ºåŠ é€Ÿæ•ˆæœå±•ç¤ºï¼Œå·¦ä¾§ä¸º OneFlow ï¼Œç¡¬ä»¶ä¸º NVIDIA A100 40G

## è½¯ä»¶ä¾èµ–

è¯·ç›´æ¥ä½¿ç”¨ä¸‹é¢å‘½ä»¤å®‰è£… OneFlow nightly CUDA 11.7 ç‰ˆæœ¬çš„å®‰è£…åŒ…ï¼š

```shell
python3 -m pip install --pre oneflow -f https://staging.oneflow.info/branch/master/cu117
```

## å®‰è£…æ–¹å¼

ä¸‹è½½æˆ– clone æœ¬é¡¹ç›®åï¼Œåœ¨é¡¹ç›®ç›®å½•ä¸­è¿è¡Œï¼š

```shell
python3 -m pip install -e .
```

æˆ–

```shell
python3 -m pip install git+https://github.com/Oneflow-Inc/faster-chatglm-6b.git
```

## ä»£ç è°ƒç”¨

```ipython
>>> import faster_chatglm_6b
>>> from transformers import AutoTokenizer, AutoModel
>>> tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
>>> model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).half().cuda()
>>> response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
>>> print(response)
ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
```

æ³¨æ„ï¼šè¿™é‡Œå’Œ[THUDM/chatglm-6b å®˜æ–¹ç¤ºä¾‹](https://huggingface.co/THUDM/chatglm-6b#%E4%BB%A3%E7%A0%81%E8%B0%83%E7%94%A8)ä¸åŒçš„æ˜¯ï¼Œåœ¨å®˜æ–¹ç¤ºä¾‹çš„æœ€å‰é¢å¢åŠ äº†ä¸€è¡Œ `import faster_chatglm_6b`ï¼Œè¿™æ ·å°±æŠŠåå°åˆ‡æ¢æˆäº† OneFlowã€‚

ä¸‹é¢è§£é‡Šä¸€ä¸‹ï¼Œè¿™ä¸€è¡Œä»£ç ä¸»è¦å‘ç”Ÿäº†ä»€ä¹ˆ:

1. è®¾ç½® OneFlow çš„ä¸€äº›ç¯å¢ƒå˜é‡ï¼Œç”¨äºæ§åˆ¶ OneFlow æ¡†æ¶çš„è¡Œä¸ºã€‚
2. ä½¿ç”¨ OneFlow çš„ `mock_torch` æ–¹æ³•æŠŠæ‰€æœ‰çš„ PyTorch æ¨¡å—æ›¿æ¢æˆå¯¹åº”çš„ OneFlow æ¨¡å—ã€‚
3. åˆ©ç”¨ transformers æ¨¡å—çš„åŠ¨æ€æ¨¡å—å·¥å…·ï¼ŒæŠŠæ‰¾åˆ°çš„åŸ ChatGLM-6B ä¸­çš„ `ChatGLMForConditionalGeneration` æ¨¡å—æ›¿æ¢æˆç»è¿‡ OneFlow ä¼˜åŒ–çš„ `ChatGLMForConditionalGeneration` æ¨¡å—ã€‚

è¿™ä¸€è¡Œçš„è¯¦ç»†è¡Œä¸ºï¼Œè¯·å‚è€ƒ `faster_chatglm_6b/__init__.py`ã€‚

## æ›´å¤šæ¼”ç¤º

æˆ‘ä»¬æ¨¡ä»¿ https://github.com/THUDM/ChatGLM-6B é¡¹ç›®ä¹Ÿæä¾›äº†å‘½ä»¤è¡Œå’Œç½‘é¡µç‰ˆçš„æ¼”ç¤ºï¼Œè¯·å‚è€ƒ `examples` ç›®å½•ä¸‹çš„æ–‡ä»¶ã€‚

```shell
examples/
â”œâ”€â”€ demo.py     # å•è½®å¯¹è¯æ¼”ç¤º
â”œâ”€â”€ cli_demo.py # å‘½ä»¤è¡Œå¤šè½®å¯¹è¯æ¼”ç¤º
â””â”€â”€ web_demo.py # ç½‘é¡µç‰ˆå¯¹è¯æ¼”ç¤º
```

## åŸç†åˆ†æ

åœ¨å‰é¢çš„æ¼”ç¤ºä¸­ï¼Œæˆ‘ä»¬æåˆ°ï¼Œé€šè¿‡å¢åŠ äº†ä¸€è¡Œ `import faster_chatglm_6b`ï¼Œå°±å¯ä»¥æŠŠChatGLMè¿è¡Œçš„åå°åˆ‡æ¢æˆ OneFlowï¼Œå…·ä½“æ¥è¯´è¿™ä¸€è¡Œä»£ç ï¼š

1. è®¾ç½®äº† OneFlow çš„ä¸€äº›ç¯å¢ƒå˜é‡ï¼Œç”¨äºæ§åˆ¶ OneFlow æ¡†æ¶çš„è¡Œä¸ºã€‚
2. ä½¿ç”¨ OneFlow çš„ `mock_torch` æ–¹æ³•æŠŠæ‰€æœ‰çš„ PyTorch æ¨¡å—æ›¿æ¢æˆå¯¹åº”çš„ OneFlow æ¨¡å—ã€‚
3. åˆ©ç”¨ transformers æ¨¡å—çš„åŠ¨æ€æ¨¡å—å·¥å…·ï¼ŒæŠŠæ‰¾åˆ°çš„åŸ ChatGLM-6B ä¸­çš„ `ChatGLMForConditionalGeneration` æ¨¡å—æ›¿æ¢æˆç»è¿‡ OneFlow ä¼˜åŒ–çš„ `ChatGLMForConditionalGeneration` æ¨¡å—ã€‚

è¯¦ç»†è¡Œä¸ºï¼Œè¯·å‚è€ƒ `faster_chatglm_6b/__init__.py`ã€‚

å¦å¤–æˆ‘ä»¬å¯¹ChatGLMéª¨å¹²ç½‘çš„ä¸€äº›æ¨¡å—è¿›è¡Œäº†ä¼˜åŒ–ï¼Œè¿™äº›æ¨¡å—åŒ…æ‹¬ï¼š

- fast_gelu

æˆ‘ä»¬é‡‡ç”¨`fast_gelu`æ›¿æ¢äº†`gelu_impl`çš„å®ç°ã€‚

- fused_apply_rotary_emb

ChatGLMä¸­ï¼Œæ¯ä¸€ä¸ªGLMBlockå±‚çš„SelfAttentionæ¨¡å—ï¼Œéƒ½éœ€è¦å¯¹å¼ é‡query_layer, key_layerè¿›è¡Œæ—‹è½¬ä½ç½®åµŒå…¥æ“ä½œï¼ˆRotary Position Embeddingï¼‰ï¼Œè¯¥æ“ä½œæ ¹æ®å½“å‰ä½ç½®å’Œä¸åŒé¢‘ç‡çš„åç§»é‡å°†ä½ç½®è®¡ç®—ä¸ºä¸€ä¸ªçŸ¢é‡ï¼Œç„¶åå°†è¿™ä¸ªçŸ¢é‡åº”ç”¨äºä¸€ä¸ªçŸ©é˜µï¼Œè¯¥çŸ©é˜µè¡¨ç¤ºä¸ºç”±ä¸€ç»„ä¸€ä¸ªå¤æ•°è§’åº¦å¯¹ç»„æˆçš„æ—‹è½¬çŸ©é˜µã€‚æˆ‘ä»¬é‡‡ç”¨`fused_apply_rotary_emb`å°†ä¸Šè¿°è¿ç®—æ”¾åˆ°ä¸€ä¸ªkernelä¸­å®Œæˆã€‚

- fused_attention_concat_past_key_value

æˆ‘ä»¬å°†attention functionä¸­keyå’Œvalueä¸è¿‡å»ä¿¡æ¯æ‹¼æ¥çš„è¿‡ç¨‹èåˆåˆ°ä¸€ä¸ª`fused_attention_concat_past_key_value`ä¸­è¿›è¡Œä¸€æ¬¡è®¡ç®—ã€‚

- fused_multi_head_attention_inference_v2

æˆ‘ä»¬å°†å¤šå¤´æ³¨æ„åŠ›è®¡ç®—èåˆåˆ°`fused_multi_head_attention_inference_v2`ä¸­ï¼ŒåŒ…æ‹¬äº†è®¡ç®—å‡ºæ¯ä¸ªquery vectorå¯¹åº”æ‰€æœ‰key vectorsçš„æ³¨æ„åŠ›åˆ†æ•°ï¼›å¯¹æ³¨æ„åŠ›åˆ†æ•°è¿›è¡Œç¼©æ”¾ï¼Œä½¿ç”¨softmaxå‡½æ•°å°†å…¶è½¬æ¢ä¸ºæ³¨æ„åŠ›æƒé‡ï¼›ä½¿ç”¨æ³¨æ„åŠ›æƒé‡å¯¹æ‰€æœ‰value vectorsè¿›è¡ŒåŠ æƒæ±‚å’Œç­‰æ“ä½œã€‚

- fused_fast_gelu_mul

æˆ‘ä»¬å°†`x1 * F.gelu(x2)`çš„æ“ä½œèåˆæˆ`fused_fast_gelu_mul`ç®—å­ã€‚

- fused_matmul_bias

æˆ‘ä»¬ä½¿ç”¨`fused_matmul_bias`å°†`4h_to_h`çš„æ“ä½œå’Œç¬¬äºŒä¸ªæ®‹å·®è¿æ¥æ“ä½œèåˆèµ·æ¥è®¡ç®—ã€‚

## æµ‹è¯•ç»“æœ

æˆ‘ä»¬å¯¹ä¼˜åŒ–åçš„æ•ˆæœè¿›è¡Œäº†å®šé‡çš„æµ‹è¯•ï¼Œæµ‹è¯•è„šæœ¬è¯·å‚è€ƒï¼šexamples/benchmark.pyã€‚

æµ‹è¯•åœ¨NVIDIA A100 40Gä¸­è¿›è¡Œï¼Œä¸€å…±æµ‹è¯•äº†4ç»„å‚æ•°ï¼Œæ¯ç»„å‚æ•°éƒ½è¿è¡Œäº†5æ¬¡ï¼Œæµ‹è¯•ç»“æœå¦‚ä¸‹ï¼š

| Backend | Quantization | Duration(s) | average  |
| ------- | ------------ | ----------- | -------- |
| OneFlow | Disable      | 30.58       | 1.53s/it |
| OneFlow | Disable      | 30.96       | 1.55s/it |
| OneFlow | Disable      | 30.52       | 1.53s/it |
| OneFlow | Disable      | 31.13       | 1.56s/it |
| OneFlow | Disable      | 30.71       | 1.54s/it |
| OneFlow | Enable       | 33.30       | 1.66s/it |
| OneFlow | Enable       | 33.68       | 1.68s/it |
| OneFlow | Enable       | 26.06       | 1.30s/it |
| OneFlow | Enable       | 33.77       | 1.69s/it |
| OneFlow | Enable       | 29.30       | 1.46s/it |
| Torch   | Disable      | 110.65      | 5.53s/it |
| Torch   | Disable      | 112.23      | 5.61s/it |
| Torch   | Disable      | 135.04      | 6.75s/it |
| Torch   | Disable      | 109.02      | 5.45s/it |
| Torch   | Disable      | 114.88      | 5.74s/it |
| Torch   | Enable       | 161.77      | 8.09s/it |
| Torch   | Enable       | 162.93      | 8.15s/it |
| Torch   | Enable       | 185.70      | 9.29s/it |
| Torch   | Enable       | 159.99      | 8.00s/it |
| Torch   | Enable       | 161.69      | 8.08s/it |

## FAQ



## TODOs

1. OneFlow æ”¯æŒ skip_init åŠŸèƒ½ä¹‹åï¼Œ`faster_chatglm_6b/__init__.py`ä¸­ç§»é™¤`new_skip_init`ã€‚
2. ç§»é™¤ `cli_demo.py` å’Œ `web_demo.py` ä¸­å¯¹ `torch.no_grad` çš„ä¾èµ–ã€‚

